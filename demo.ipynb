{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded openai api key.\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter the file path or URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_path = input(\"Enter the file path or URL: \")\n",
    "text_path = \"example/example.pdf\"\n",
    "\n",
    "text = get_text(text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memorize the data, the same document will only be processed once and cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cached memories.\n"
     ]
    }
   ],
   "source": [
    "memory_path = memorize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question: What is this article about?\n",
      "\n",
      "\u001b[92mThis article is about a new model architecture called the Transformer that uses solely attention mechanisms for sequence transduction, dispensing with recurrence and convolutions. The article describes the architecture and its advantages over traditional models, as well as its successful application to machine translation tasks and English constituency parsing. The article also includes visualizations of the attention mechanism in action.\u001b[0m\n",
      "\n",
      "Enter your question: 这篇文章讲了什么？\n",
      "\n",
      "\u001b[92m这篇文章讲了一个新的网络结构——Transformer，它完全基于注意力机制，不使用循环或卷积。文章提出，这种结构能够更好地处理序列转换问题，能够在翻译任务中获得更好的性能，并且在训练时需要更少的时间。文章还提到，该模型已被成功应用于英语成分分析，并取得了良好的结果。作者们还希望未来能够将这种结构应用于其他任务，并探索在处理大型输入和输出时如何使用局部的注意力机制。\u001b[0m\n",
      "\n",
      "Enter your question: How self-attention is calculated?\n",
      "\n",
      "\u001b[92mSelf-attention is calculated using a multi-head self-attention mechanism in which queries, keys, and values are linearly projected h times with different, learned linear projections to dimensions dk, dk, and dv, respectively. The attention function maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values, with the weight assigned to each value computed by a compatibility function of the query with the corresponding key. The scaled dot-product attention mechanism is used, in which the dot products of the query with all keys are computed, divided by √dk, and a softmax function is applied to obtain the weights on the values. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. The positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. The training regime involves using the Adam optimizer and batching sentences by approximate sequence length. The models are trained on a machine with multiple GPUs for a specific number of steps or hours.\u001b[0m\n",
      "\n",
      "Enter your question: 自注意力是如何计算的？\n",
      "\n",
      "\u001b[92mThe self-attention mechanism in the Transformer model computes representations of its input and output using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The attention function maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. The positional encoding in the model corresponds to a sinusoid to allow the model to easily learn to attend by relative positions. The use of self-attention yields more interpretable models, as individual attention heads learn to perform different tasks, many of which appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat(memory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
